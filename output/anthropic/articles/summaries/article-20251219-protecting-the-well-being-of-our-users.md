---
title: "Protecting the well-being of our users"
published: "2025-12-19"
collected_at: "2026-01-19T14:23:17.837Z"
url: "https://anthropic.com/news/protecting-well-being-of-users"
source: "news"
source_medium: "Anthropic News"
language: "ja"
---

# Protecting the well-being of our users

## Key Points
- Claudeはユーザーの感情的なサポートなど様々な用途で利用されるため、ユーザーのウェルビーイングを確保するため、共感とAIとしての限界を正直に伝えるよう訓練されています。特に自殺や自傷行為に関する会話、および「ごますり」（sycophancy）の削減に焦点を当てています。
- Claudeは専門的な医療アドバイスの代替ではなく、ユーザーが自殺や自傷行為の考えを表明した場合、共感的に対応しつつ、ヘルプラインやメンタルヘルス専門家など人間によるサポートへの誘導を優先します。
- Claude.aiでは、ユーザーが専門的な支援を必要とする可能性を検出する分類器を導入し、危機的状況ではThroughLineが提供する専門機関（例: 日本のLife Link、米国の988 Lifeline）へのバナーを表示します。
- 最新モデル（Claude Opus 4.5, Sonnet 4.5, Haiku 4.5）は、自殺・自傷行為に関する単一ターン応答で98.6%〜99.3%、複数ターン会話で最大86%の適切な応答率を示し、以前のモデルより改善されています。また、「ごますり」の削減においても、Opus 4.1より70-85%低いスコアを記録し、オープンソースのPetri評価セットで他のフロンティアモデルを上回っています。
- Claude.aiの利用は18歳以上に限定されており、未成年ユーザーを特定するための分類器を開発中です。Anthropicは、ユーザーの安全保護のため、今後も保護機能の構築、評価の改善、透明性の維持、業界内の専門家との協力を継続していくと表明しています。